# -*- coding: utf-8 -*-
"""Social_Media_Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BJRHE3sGd7CPkiXXACTd7k3pNa7biY8m
"""

from google.colab import drive
drive.mount('/content/drive')

"""Reading CSV file"""

import pandas as pd
import numpy as np

data = pd.read_csv('/content/drive/MyDrive/Review.csv')

print(data) # Reviews

data.replace({'sentiment':{'Positive':1}},inplace=True)

data.replace({'sentiment':{'Negative':0}},inplace=True)

print(data)

"""The data has equal features of each label"""

data['sentiment'].value_counts()

"""Creating seperate data frames for reviews and labels"""

X_data = data['review']
print(X_data)

y = data['sentiment']
print(y)

"""# Cleaning of the data"""

# Tokenize
# "I am a python dev" -> ["I", "am", "a", "python", "dev"]

from nltk.tokenize import RegexpTokenizer
# NLTK -> Tokenize -> RegexpTokenizer

# Stemming
# "Playing" -> "Play"
# "Working" -> "Work"

from nltk.stem import PorterStemmer
# NLTK -> Stem -> Porter -> PorterStemmer
import string
from nltk.corpus import stopwords
# NLTK -> Corpus -> stopwords

# Downloading the stopwords
import nltk
nltk.download('stopwords')
#! pip install emoji==1.7

import emoji
from textblob import TextBlob

tokenizer = RegexpTokenizer(r"\w+")
en_stopwords = set(stopwords.words('english'))
ps = PorterStemmer()

def getCleanedText(text):
  try:
    if text is None:
          return ""

    if not isinstance(text, str):
          text = str(text)

    # tokenizing
    text_without_emojis = ''.join(c for c in text if c not in emoji.UNICODE_EMOJI)
    tokens = tokenizer.tokenize(text_without_emojis)
    tokens = [char for char in tokens if char not in string.punctuation]
    new_tokens = [token for token in tokens if token not in en_stopwords]
    stemmed_tokens = [ps.stem(tokens) for tokens in new_tokens]
    clean_text = " ".join(stemmed_tokens)
    return clean_text
  except Exception as e:
        print(f"Error processing text: {text}")
        print(f"Error message: {str(e)}")
        return ""

pred = ["i have happy news"]
X_Clean = [getCleanedText(i) for i in X_data]
xt_clean = [getCleanedText(i) for i in pred]

print(X_Clean)

"""# Vectorize"""

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer()
# "I am PyDev" -> "i am", "am Pydev"

X = cv.fit_transform(X_Clean).toarray()

print(X)

print(cv.get_feature_names_out())

Xt_vect = cv.transform(xt_clean).toarray()

print(Xt_vect)

"""# Multinomial Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

mn = MultinomialNB()

mn.fit(X, y)

"""Sample prediction"""

y_pred = mn.predict(Xt_vect)

print(y_pred)

"""# Saving the model"""

import pickle

filename = 'trained_model.sav'
pickle.dump(mn, open(filename, 'wb'))

# loading saved model
loaded_model = pickle.load(open('/content/trained_model.sav', "rb"))

def predict(text):
  X_new = [getCleanedText(i) for i in text]
  X_pred = cv.transform(X_new).toarray()
  prediction = loaded_model.predict(X_pred)

  if(prediction[0] == 0):
    return('Negative tweet')
  else:
    return('Positive tweet')

predict(['Leo has wonderful bgm score'])

predict(['his performance is legendary'])

predict(['its unbelievable that in the 21st century wed need something like this. again. #neverump  #xenophobia '])

predict(['The Government Media Office in Gaza says over 700 Palestinians were killed by Israeli airstrikes in the besieged Gaza Strip over the last 24 hours'])

predict(['He is a Poor child lost his father and his brother, after IOF bombed their house in Gaza'])

predict(["desai is a sore loser ,😂😂🤣🤣🤜🏼🤥😞😋😔🙁 he cannot surviv"])

predict(['Yamazaki gives G-fans plenty of reasons to see “Godzilla Minus One” in theaters. He’s got a clear eye for action and a firm grasp on feel-good, saber-rattling melodrama. '])

predict(['not satisfied'])

predict(['I am not feeling well'])

predict(['i dont want to live anymore'])
